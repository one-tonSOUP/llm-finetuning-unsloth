{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Fine Tune LLM**\n",
        "---\n",
        " - **Use Case** : Not to add new knowledge, it's to adapt specific tasks and domains.\n",
        " - Use a model which is for a particular task that is has already knowledge on it inside it.\n",
        " - **Popular Supervised Fine Tuning techniques** :\n",
        "\t1. Full Fine Tuning: Involves re-training all the parameters of a pre-trained model on a instruction data set. Not use full because neither companies nor us have these many GPUs and this may lead to forgetting their original information as well.\n",
        "\t2. Low Rank Adaptation / Lora : Parameter efficient fine-tuning technique. It basically freezes the actual weights or original weights and they introduce a small adapters. Adapters are nothing but low rank metrices at each targeted layer. So this allows Lora to train a number of parameters that drastically lower than full fine-tuning, less than 1%. Reduces both memory usage and training time. Non-destructive because the model will not forget everything that it has learned. So it can be switched or combined and it has extra power.\n",
        "\t3. Quantized Low Rank Adaptation / Qlora : Extention of Lora and provides 33% memory reduction.\n",
        "\n",
        "---\n",
        "Library to be used - `Unsloth`. More compatible with Llama models. We will basically fine tume `Llama 3.2 3b`. We'll used Qlora for fine-tuning on `finetome-100k` dataset."
      ],
      "metadata": {
        "id": "0EKk6pMItr_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup\n",
        "\n",
        "To install the required packages, run:\n",
        "\n",
        "```bash\n",
        "pip install --quiet unsloth transformers trl\n",
        "```\n",
        "\n",
        " 1. **`unsloth`:** This library provides optimized kernels and integrations to significantly speed up the training and fine-tuning of large language models, especially on limited hardware. It integrates with methods like LoRA and QLoRA for efficient parameter-efficient fine-tuning.\n",
        " 2. **`transformers`:** Developed by Hugging Face, this library provides state-of-the-art pre-trained models for various tasks in natural language processing (NLP), computer vision, and audio processing. It offers a unified API to load, use, and train these models, serving as a foundational component for many LLM projects.\n",
        " 3. **`trl` (Transformer Reinforcement Learning):** This library, also from Hugging Face, is a full-stack tool for fine-tuning and aligning transformer language models using methods like Supervised Fine-Tuning (SFT), Proximal Policy Optimization (PPO), Direct Preference Optimization (DPO), and Reward Modeling. It is built on top of the transformers library and integrates with unsloth for accelerated training."
      ],
      "metadata": {
        "id": "wrWVfur5uJIo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cqFtEr-Aibg4"
      },
      "outputs": [],
      "source": [
        "pip install --quiet unsloth transformers trl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries\n",
        "\n",
        "These are the core libraries used for fine-tuning a language model:\n",
        "\n",
        "- `torch`: PyTorch framework for tensor operations and model training.\n",
        "- `unsloth.FastLanguageModel`: Efficient wrapper for loading and fine-tuning large language models with QLoRA.\n",
        "- `datasets.load_dataset`: Utility from Hugging Face to load and manage datasets easily.\n",
        "- `trl.SFTTrainer`: Supervised fine-tuning trainer for reward-based or instruction-tuned models.\n",
        "- `transformers.TrainingArguments`: Configuration class for setting training hyperparameters and logging.\n",
        "- `unsloth.chat_templates`: Provides chat formatting tools like get_chat_template and standardize_sharegpt for aligning data with model expectations.\n"
      ],
      "metadata": {
        "id": "CoONhDYPwsl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth.chat_templates import get_chat_template, standardize_sharegpt"
      ],
      "metadata": {
        "id": "MN72YxUKi3m8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe72c8fc-1fdd-4bd1-bf12-8960812f29fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of Model Loading Parameters :**\n",
        "\n",
        "| Code Component                                 | Explanation                           | Possible Alternatives                | Effect on Fine-Tuning                                                                                   |\n",
        "| ---------------------------------------------- | ------------------------------------- | ------------------------------------ | ------------------------------------------------------------------------------------------------------- |\n",
        "| `model_name = \"unsloth/Llama-3.2-3B-Instruct\"` | Loads the specified pre-trained model | Any Llama or HF model                | Changes quality, VRAM needs, and training speed                                                         |\n",
        "| `max_seq_length = 2048`                        | Sets max tokens per input sequence    | 512, 1024, 4096, 8192 (if supported) | Higher values allow longer inputs but use more memory                                                   |\n",
        "| `load_in_4bit = True`                          | Loads the model in 4-bit quantization | False (fp16/fp32 loading)            | 4-bit reduces VRAM and enables QLoRA; full precision uses more memory but can be slightly more accurate |\n"
      ],
      "metadata": {
        "id": "TD-sk3bZzjs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    max_seq_length=2048,\n",
        "    load_in_4bit=True\n",
        " )"
      ],
      "metadata": {
        "id": "qLr_q0bQkLH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of LoRA Configuration Parameters :**\n",
        "\n",
        "| Code Component           | Explanation                                              | Possible Alternatives                                       | Effect on Fine-Tuning                                               |\n",
        "| ------------------------ | -------------------------------------------------------- | ----------------------------------------------------------- | ------------------------------------------------------------------- |\n",
        "| `r = 16`                 | LoRA rank determining size of trainable adapter matrices | 4, 8, 32, 64                                                | Higher rank improves capacity but increases VRAM and training time  |\n",
        "| `target_modules = [...]` | Specifies which model layers receive LoRA adapters       | Subset or different modules like only `q_proj` and `v_proj` | More modules increase learnability but also memory and compute cost |\n",
        "| `model` (input argument) | Applies LoRA adapters to the loaded model                | Any previously loaded model                                 | Determines which base model is being adapted                        |\n"
      ],
      "metadata": {
        "id": "0XsMaHYP0UzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model, r=16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6LxacmIkoAn",
        "outputId": "d67c4ce9-d562-46a9-f061-61a92aed5e86"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.11.3 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below line applies the Llama-3.1 chat formatting style to the tokenizer. It tells the tokenizer how to structure conversations (system, user, assistant turns) in the format the model was trained on. You could choose other templates such as \"llama-2\", \"chatml\", or \"mistral\"; choosing the wrong one may hurt fine-tuning quality because the model expects a specific conversation structure."
      ],
      "metadata": {
        "id": "gAD_X4FB0mlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_chat_template(tokenizer, chat_template=\"llama-3.1\")"
      ],
      "metadata": {
        "id": "c17ztfBml0gm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Dataset\n",
        "\n",
        "**`FineTome-100k dataset`**: A high-quality collection of 100,000 instructionâ€“response pairs designed for supervised fine-tuning of LLMs. The dataset focuses on clean, well-structured prompts and helpful assistant answers, making it suitable for improving general instruction-following behavior. Using it as the training split means the model will learn from all available examples.\n",
        "\n",
        "Dataset link: https://huggingface.co/datasets/mlabonne/FineTome-100k"
      ],
      "metadata": {
        "id": "4KD3zrJb1I1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"mlabonne/FineTome-100k\", split=\"train\")"
      ],
      "metadata": {
        "id": "htteoNMbl0jH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below line converts the dataset into the ShareGPT-style conversation format, ensuring all samples follow a consistent multi-turn chat structure. This standardization helps the model correctly interpret userâ€“assistant roles during fine-tuning."
      ],
      "metadata": {
        "id": "-r_4Vhb31UWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = standardize_sharegpt(dataset)"
      ],
      "metadata": {
        "id": "I5pyic5al0lF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tm8jNgLUl0nS",
        "outputId": "12ec135c-44b6-445c-bc97-54a4e082f0e1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['conversations', 'source', 'score'],\n",
              "    num_rows: 100000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EUKbxWDmgxf",
        "outputId": "ee4294f0-21b4-4ce0-9886-c76aba1c485b",
        "collapsed": true
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'conversations': [{'content': 'Explain what boolean operators are, what they do, and provide examples of how they can be used in programming. Additionally, describe the concept of operator precedence and provide examples of how it affects the evaluation of boolean expressions. Discuss the difference between short-circuit evaluation and normal evaluation in boolean expressions and demonstrate their usage in code. \\n\\nFurthermore, add the requirement that the code must be written in a language that does not support short-circuit evaluation natively, forcing the test taker to implement their own logic for short-circuit evaluation.\\n\\nFinally, delve into the concept of truthiness and falsiness in programming languages, explaining how it affects the evaluation of boolean expressions. Add the constraint that the test taker must write code that handles cases where truthiness and falsiness are implemented differently across different programming languages.',\n",
              "   'role': 'user'},\n",
              "  {'content': 'Boolean operators are logical operators used in programming to manipulate boolean values. They operate on one or more boolean operands and return a boolean result. The three main boolean operators are \"AND\" (&&), \"OR\" (||), and \"NOT\" (!).\\n\\nThe \"AND\" operator returns true if both of its operands are true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) and (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"OR\" operator returns true if at least one of its operands is true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) or (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"NOT\" operator negates the boolean value of its operand. It returns true if the operand is false, and false if the operand is true. For example:\\n\\n```python\\nx = 5\\nresult = not (x > 10)  # This expression evaluates to True\\n```\\n\\nOperator precedence refers to the order in which operators are evaluated in an expression. It ensures that expressions are evaluated correctly. In most programming languages, logical AND has higher precedence than logical OR. For example:\\n\\n```python\\nresult = True or False and False  # This expression is evaluated as (True or (False and False)), which is True\\n```\\n\\nShort-circuit evaluation is a behavior where the second operand of a logical operator is not evaluated if the result can be determined based on the value of the first operand. In short-circuit evaluation, if the first operand of an \"AND\" operator is false, the second operand is not evaluated because the result will always be false. Similarly, if the first operand of an \"OR\" operator is true, the second operand is not evaluated because the result will always be true.\\n\\nIn programming languages that support short-circuit evaluation natively, you can use it to improve performance or avoid errors. For example:\\n\\n```python\\nif x != 0 and (y / x) > 10:\\n    # Perform some operation\\n```\\n\\nIn languages without native short-circuit evaluation, you can implement your own logic to achieve the same behavior. Here\\'s an example in pseudocode:\\n\\n```\\nif x != 0 {\\n    if (y / x) > 10 {\\n        // Perform some operation\\n    }\\n}\\n```\\n\\nTruthiness and falsiness refer to how non-boolean values are evaluated in boolean contexts. In many programming languages, non-zero numbers and non-empty strings are considered truthy, while zero, empty strings, and null/None values are considered falsy.\\n\\nWhen evaluating boolean expressions, truthiness and falsiness come into play. For example:\\n\\n```python\\nx = 5\\nresult = x  # The value of x is truthy, so result is also truthy\\n```\\n\\nTo handle cases where truthiness and falsiness are implemented differently across programming languages, you can explicitly check the desired condition. For example:\\n\\n```python\\nx = 5\\nresult = bool(x)  # Explicitly converting x to a boolean value\\n```\\n\\nThis ensures that the result is always a boolean value, regardless of the language\\'s truthiness and falsiness rules.',\n",
              "   'role': 'assistant'}],\n",
              " 'source': 'infini-instruct-top-500k',\n",
              " 'score': 5.212620735168457}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of the next step.**\n",
        "\n",
        " - Take batch of examples â†’ `dataset.map(..., batched=True)`\n",
        "\n",
        "Converts multiple dataset entries at once.\n",
        "\n",
        " - Extract conversations â†’ `examples[\"conversations\"]`\n",
        "\n",
        "Reads the list of chat turns for each example.\n",
        "\n",
        " - Apply chat template â†’ `tokenizer.apply_chat_template(convo, tokenize=False)`\n",
        "\n",
        "Formats each conversation into a single text string.\n",
        "\n",
        " - Collect formatted texts â†’ list comprehension\n",
        "\n",
        "Builds a list of formatted conversation strings.\n",
        "\n",
        " - Store result as \"text\" field â†’ `{\"text\": [...]}`\n",
        "\n",
        "Adds a new field used for training input.\n",
        "\n",
        " - Return updated dataset\n",
        "\n",
        "Produces a version of the dataset ready for tokenization and fine-tuning."
      ],
      "metadata": {
        "id": "AesS4OH52W-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(\n",
        "    lambda examples: {\n",
        "        \"text\": [\n",
        "            tokenizer.apply_chat_template(convo, tokenize=False)\n",
        "            for convo in examples[\"conversations\"]\n",
        "        ]\n",
        "    },\n",
        "    batched=True\n",
        ")"
      ],
      "metadata": {
        "id": "WgnYijk2mkhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sERgQz6vm-wx",
        "outputId": "b5d8858b-2f8d-4450-a5e6-d14f7d4ddc1d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['conversations', 'source', 'score', 'text'],\n",
              "    num_rows: 100000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJ89b247nB1W",
        "outputId": "e4c40d38-ba1b-405f-acc3-a8b35f61fddf",
        "collapsed": true
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'conversations': [{'content': 'Explain what boolean operators are, what they do, and provide examples of how they can be used in programming. Additionally, describe the concept of operator precedence and provide examples of how it affects the evaluation of boolean expressions. Discuss the difference between short-circuit evaluation and normal evaluation in boolean expressions and demonstrate their usage in code. \\n\\nFurthermore, add the requirement that the code must be written in a language that does not support short-circuit evaluation natively, forcing the test taker to implement their own logic for short-circuit evaluation.\\n\\nFinally, delve into the concept of truthiness and falsiness in programming languages, explaining how it affects the evaluation of boolean expressions. Add the constraint that the test taker must write code that handles cases where truthiness and falsiness are implemented differently across different programming languages.',\n",
              "   'role': 'user'},\n",
              "  {'content': 'Boolean operators are logical operators used in programming to manipulate boolean values. They operate on one or more boolean operands and return a boolean result. The three main boolean operators are \"AND\" (&&), \"OR\" (||), and \"NOT\" (!).\\n\\nThe \"AND\" operator returns true if both of its operands are true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) and (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"OR\" operator returns true if at least one of its operands is true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) or (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"NOT\" operator negates the boolean value of its operand. It returns true if the operand is false, and false if the operand is true. For example:\\n\\n```python\\nx = 5\\nresult = not (x > 10)  # This expression evaluates to True\\n```\\n\\nOperator precedence refers to the order in which operators are evaluated in an expression. It ensures that expressions are evaluated correctly. In most programming languages, logical AND has higher precedence than logical OR. For example:\\n\\n```python\\nresult = True or False and False  # This expression is evaluated as (True or (False and False)), which is True\\n```\\n\\nShort-circuit evaluation is a behavior where the second operand of a logical operator is not evaluated if the result can be determined based on the value of the first operand. In short-circuit evaluation, if the first operand of an \"AND\" operator is false, the second operand is not evaluated because the result will always be false. Similarly, if the first operand of an \"OR\" operator is true, the second operand is not evaluated because the result will always be true.\\n\\nIn programming languages that support short-circuit evaluation natively, you can use it to improve performance or avoid errors. For example:\\n\\n```python\\nif x != 0 and (y / x) > 10:\\n    # Perform some operation\\n```\\n\\nIn languages without native short-circuit evaluation, you can implement your own logic to achieve the same behavior. Here\\'s an example in pseudocode:\\n\\n```\\nif x != 0 {\\n    if (y / x) > 10 {\\n        // Perform some operation\\n    }\\n}\\n```\\n\\nTruthiness and falsiness refer to how non-boolean values are evaluated in boolean contexts. In many programming languages, non-zero numbers and non-empty strings are considered truthy, while zero, empty strings, and null/None values are considered falsy.\\n\\nWhen evaluating boolean expressions, truthiness and falsiness come into play. For example:\\n\\n```python\\nx = 5\\nresult = x  # The value of x is truthy, so result is also truthy\\n```\\n\\nTo handle cases where truthiness and falsiness are implemented differently across programming languages, you can explicitly check the desired condition. For example:\\n\\n```python\\nx = 5\\nresult = bool(x)  # Explicitly converting x to a boolean value\\n```\\n\\nThis ensures that the result is always a boolean value, regardless of the language\\'s truthiness and falsiness rules.',\n",
              "   'role': 'assistant'}],\n",
              " 'source': 'infini-instruct-top-500k',\n",
              " 'score': 5.212620735168457,\n",
              " 'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nExplain what boolean operators are, what they do, and provide examples of how they can be used in programming. Additionally, describe the concept of operator precedence and provide examples of how it affects the evaluation of boolean expressions. Discuss the difference between short-circuit evaluation and normal evaluation in boolean expressions and demonstrate their usage in code. \\n\\nFurthermore, add the requirement that the code must be written in a language that does not support short-circuit evaluation natively, forcing the test taker to implement their own logic for short-circuit evaluation.\\n\\nFinally, delve into the concept of truthiness and falsiness in programming languages, explaining how it affects the evaluation of boolean expressions. Add the constraint that the test taker must write code that handles cases where truthiness and falsiness are implemented differently across different programming languages.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nBoolean operators are logical operators used in programming to manipulate boolean values. They operate on one or more boolean operands and return a boolean result. The three main boolean operators are \"AND\" (&&), \"OR\" (||), and \"NOT\" (!).\\n\\nThe \"AND\" operator returns true if both of its operands are true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) and (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"OR\" operator returns true if at least one of its operands is true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) or (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"NOT\" operator negates the boolean value of its operand. It returns true if the operand is false, and false if the operand is true. For example:\\n\\n```python\\nx = 5\\nresult = not (x > 10)  # This expression evaluates to True\\n```\\n\\nOperator precedence refers to the order in which operators are evaluated in an expression. It ensures that expressions are evaluated correctly. In most programming languages, logical AND has higher precedence than logical OR. For example:\\n\\n```python\\nresult = True or False and False  # This expression is evaluated as (True or (False and False)), which is True\\n```\\n\\nShort-circuit evaluation is a behavior where the second operand of a logical operator is not evaluated if the result can be determined based on the value of the first operand. In short-circuit evaluation, if the first operand of an \"AND\" operator is false, the second operand is not evaluated because the result will always be false. Similarly, if the first operand of an \"OR\" operator is true, the second operand is not evaluated because the result will always be true.\\n\\nIn programming languages that support short-circuit evaluation natively, you can use it to improve performance or avoid errors. For example:\\n\\n```python\\nif x != 0 and (y / x) > 10:\\n    # Perform some operation\\n```\\n\\nIn languages without native short-circuit evaluation, you can implement your own logic to achieve the same behavior. Here\\'s an example in pseudocode:\\n\\n```\\nif x != 0 {\\n    if (y / x) > 10 {\\n        // Perform some operation\\n    }\\n}\\n```\\n\\nTruthiness and falsiness refer to how non-boolean values are evaluated in boolean contexts. In many programming languages, non-zero numbers and non-empty strings are considered truthy, while zero, empty strings, and null/None values are considered falsy.\\n\\nWhen evaluating boolean expressions, truthiness and falsiness come into play. For example:\\n\\n```python\\nx = 5\\nresult = x  # The value of x is truthy, so result is also truthy\\n```\\n\\nTo handle cases where truthiness and falsiness are implemented differently across programming languages, you can explicitly check the desired condition. For example:\\n\\n```python\\nx = 5\\nresult = bool(x)  # Explicitly converting x to a boolean value\\n```\\n\\nThis ensures that the result is always a boolean value, regardless of the language\\'s truthiness and falsiness rules.<|eot_id|>'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SFTTrainer Setup\n",
        "\n",
        "| Step | What It Does | Code Piece |\n",
        "|------|--------------|------------|\n",
        "| 1 | Provide the model that will be fine-tuned | `model = model` |\n",
        "| 2 | Provide the tokenizer used for formatting and tokenization | `tokenizer = tokenizer` |\n",
        "| 3 | Give the dataset to train on | `train_dataset = dataset` |\n",
        "| 4 | Specify which column contains the input text | `dataset_text_field = \"text\"` |\n",
        "| 5 | Set maximum token length for each training sample | `max_seq_length = 2048` |\n",
        "| 6 | Begin setting training configuration | `TrainingArguments(...)` |\n",
        "| 7 | Set per-device batch size | `per_device_train_batch_size = 2` |\n",
        "| 8 | Accumulate gradients to simulate a larger batch size | `gradient_accumulation_steps = 4` |\n",
        "| 9 | Warm up the learning rate at the start of training | `warmup_steps = 5` |\n",
        "| 10 | Define total number of training steps | `max_steps = 60` |\n",
        "| 11 | Set how fast the model learns | `learning_rate = 2e-4` |\n",
        "| 12 | Use FP16 if BF16 is not supported | `fp16 = not torch.cuda.is_bf16_supported()` |\n",
        "| 13 | Use BF16 if GPU supports it | `bf16 = torch.cuda.is_bf16_supported()` |\n",
        "| 14 | Log progress every step | `logging_steps = 1` |\n",
        "| 15 | Choose where to save model outputs | `output_dir = \"outputs\"` |\n",
        "| 16 | Build the supervised fine-tuning trainer | `trainer = SFTTrainer(...)` |\n"
      ],
      "metadata": {
        "id": "vMN73Y4f3Z1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 2048,\n",
        "    tokenizer = tokenizer,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        output_dir = \"outputs\"\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "N0dEMeVjnDU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bad = []\n",
        "for i, row in enumerate(dataset):\n",
        "    try:\n",
        "        _ = tokenizer.apply_chat_template(row[\"conversations\"], tokenize=False)\n",
        "    except Exception as e:\n",
        "        bad.append(i)\n",
        "\n",
        "print(\"Bad samples:\", len(bad))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4_yj4yd4V9H",
        "outputId": "6c939e8f-25d6-45d4-900d-7bbf7621f19b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bad samples: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "9-j71dmOofcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"finetuned_model\")"
      ],
      "metadata": {
        "id": "VRnCv3LToi_q"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Fine-Tuned Model for Inference\n",
        "\n",
        "| Step | What It Does | Code Piece |\n",
        "|------|--------------|------------|\n",
        "| 1 | Load the fine-tuned model from the saved directory | `model_name=\"./finetuned_model\"` |\n",
        "| 2 | Set the maximum sequence length the model should handle | `max_seq_length=2048` |\n",
        "| 3 | Load the model in 4-bit mode to reduce memory usage during inference | `load_in_4bit=True` |\n",
        "| 4 | Initialize both the model and tokenizer for inference | `inference_model, inference_tokenizer = FastLanguageModel.from_pretrained(...)` |\n"
      ],
      "metadata": {
        "id": "-UbjCZoR9ffu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inference_model, inference_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"./finetuned_model\",\n",
        "    max_seq_length=2048,\n",
        "    load_in_4bit=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WE9KcYSpooip",
        "outputId": "91d267c7-b1c5-4e39-d49b-c123e8a03906"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.11.3: Fast Llama patching. Transformers: 4.57.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running Inference\n",
        "\n",
        "This code sends a user prompt to the fine-tuned model, generates a response, and prints it.\n",
        "\n",
        "| Step | What It Does | Code Piece |\n",
        "|------|--------------|------------|\n",
        "| 1 | Define one or more text prompts to test the model | `text_prompts = [...]` |\n",
        "| 2 | Format each prompt into a chat-style input | `apply_chat_template(...)` |\n",
        "| 3 | Tokenize the formatted prompt and move it to GPU | `inference_tokenizer(...).to(\"cuda\")` |\n",
        "| 4 | Generate model output tokens with sampling | `inference_model.generate(...)` |\n",
        "| 5 | Convert generated token IDs back to readable text | `batch_decode(...)` |\n",
        "| 6 | Print the final model response | `print(response)` |\n"
      ],
      "metadata": {
        "id": "TgDGwRJ6-KeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_prompts = [\n",
        "    \"What are the key principles of investment?\"\n",
        "]\n",
        "\n",
        "for prompt in text_prompts:\n",
        "  formatted_prompt = inference_tokenizer.apply_chat_template([{\n",
        "      \"role\": \"user\",\n",
        "      \"content\": prompt\n",
        "      }], tokenize=False)\n",
        "\n",
        "  model_inputs = inference_tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "  generated_ids = inference_model.generate(\n",
        "      **model_inputs,\n",
        "      max_new_tokens=512,\n",
        "      temperature=0.7,\n",
        "      do_sample=True,\n",
        "      pad_token_id=inference_tokenizer.pad_token_id\n",
        "  )\n",
        "  response = inference_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "  print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcQyy5HFse18",
        "outputId": "a833725b-0ce9-45af-ea3d-7f34b09cd7ac"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "system\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 16 Nov 2025\n",
            "\n",
            "user\n",
            "\n",
            "What are the key principles of investment?assistant\n",
            "\n",
            "The key principles of investment are:\n",
            "\n",
            "1. Diversification: Investing in a variety of assets to reduce risk and increase potential returns.\n",
            "2. Long-term perspective: Investing for the long-term, rather than trying to make quick profits.\n",
            "3. Risk management: Understanding and managing risk to minimize potential losses.\n",
            "4. Dollar-cost averaging: Investing a fixed amount of money at regular intervals, regardless of market conditions.\n",
            "5. Compounding: Reinvesting earnings to take advantage of the power of compounding.\n",
            "6. Rebalancing: Periodically reviewing and adjusting your portfolio to maintain an optimal asset allocation.\n",
            "7. Tax efficiency: Considering the tax implications of your investments and aiming to minimize tax liabilities.\n",
            "8. Fee efficiency: Choosing investments with low fees to maximize returns.\n",
            "9. Inflation protection: Investing in assets that historically perform well during periods of inflation.\n",
            "10. Liquidity: Ensuring that your investments can be easily sold or accessed when needed.\n",
            "11. Tax optimization: Maximizing tax benefits and minimizing tax liabilities through tax-efficient investments.\n",
            "12. ESG considerations: Considering the environmental, social, and governance (ESG) impact of your investments.\n",
            "\n",
            "These principles serve as a foundation for making informed investment decisions and can help you achieve your financial goals.\n"
          ]
        }
      ]
    }
  ]
}